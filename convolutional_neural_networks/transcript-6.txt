how do we interpret and how do we see the filters that are being learned actually you can as the model trains or after the model trains also you can visualize those filters right so you can actually look at what the convolution neural network what the convolutional layers are optimizing themselves to detect in this image so here's an example of a convolutional
neural network uh learning to detect faces and you see basically three different levels of activations of filters that are being activated by this network in the beginning it's looking for basically these edges like changes in intensity value with different directions As you move up in the model you start to combine those filters and those features together that are being extract hierarchically and you start to see actually facial features start to emerge things like eyes noses and ears like we were talking about before and
then you move up even more than this and you start to see full structures start to emerge right now with cnns the key Point here that are different than machine learning is that we didn't Define any of these filters ourselves we've learned these filters by showing the model a lot of data and asking the model hey what are the filters that would be best for you to use in order to get your accuracy of detecting faces as high as possible right we only penalize it when it's not able to detect faces and we say hey keep changing your
filters until you're able to detect faces better and this is what it comes up with these are the filters of things that it it's learned to detect in order to do this task most accurately so roughly actually now this whole process is just broken down into two parts right part number one is extracting these features using those learned filters and part number two is going to be using those features to do some form of detection right in this case we're we're talking about facial detection right so there are these two
pieces right how can we take the features that are learned from phase one on the model to inform our classification phase right we can do this with a function called a softmax function what is a softmax function this is just a function that takes our n outputs at the last layer and squashes them to have a sum to one why do we do this because we're caring about classification in this case and if you want to classify something you have to have a probability distribution what is a probability distribution you have to
have mass that sums to one okay so now let's put all of this together in code to build our first endtoend convolutional neural network we can start by defining those two parts of the network that we saw in the previous slide first part is going to be the feature extraction part this is the part with all of the filters this is going to have 32 feature maps you can see right here so the first volume that comes out of this first layer will have a volume a depth of 32 features corresponding to 32 filters that have been learned in this
first layer we will downo the spatial information and then we'll pass it onto yet another convolutional layer that takes those 32 features and learns another 64 features on top of them at a smaller Dimension we're going to then flatten all of this now we can remove our spatial information we go into a one-dimensional space because now we're actually wanting to make a one-dimensional prediction on the classification so we flatten everything because we've already learned our spatial information
and then we project to a softmax function that allows us to predict our probability distribution and allows us to Output our final let's say 10 classes right if we're trying to predict numbers we would have zero to nine different classes that's 10 total classes in pytorch you'll see a very similar uh very similar uh version of this code right almost identically mirroring what you saw before right again we defined the convolutional layers 32 followed by 64 features a flattening operation that allows us to
enter the one-dimensional space and then pass into our final output dimensionality yes you say a few words about how you choose that there's three the patch is a size three and there's 32 feature Ms and layers te me that might be more of an art than a science yes yeah it it's an art but it's also uh it can be guided by a lot of uh uh intuition I guess so some intuition that I'll provide is that typically you want your features especially depending on the task you want to start out quite small but you want to preserve you want
to uh upsample spatial information as you go deeper into the model so you want to make sure that you understand first of all what are the based on your image based on your data set you want to understand what you know looking at this as a human what are the scale of the features that this model should be looking at to detect in the beginning and at the end and those will define basically those two resolutions right if you're saying okay this is a massive image that's let's say 2,00 by 2,000 pixels wide starting with 3x3 will be
too small right probably for most images right it doesn't make sense to look at that small scale if your image is 2,000 pixels wide but then there could be other applications you know it's very small and even 3x3 is is already like a significant resolution in that image right so it's very problem dependent as well yes yeah sorry you TR your your allow not if it wasn't trained on faces that were sideways this is a great question so the question was uh you know you train it on upright faces and then
you test it on faces that are turned sideways and no it it we would not expect it to do well in this case because it has learned to detect features of faces that were always upright if it showed if it was shown faces of both orientations then we would expect it to do this successfully right because we would expect that then it should learn that faces can be in either orientation so it should learn to you know pick out those features in both cases yes how would this approach work for projectable object for example if we
have a a gun bear or if we have a flue simulation like how uh is it a at all or um it's it's very problem dependent but again we'd want to make sure that we can learn it from data right so in even in these more Dynamic cases right we'll see some examples later in the in the deck right of of these types of situations as well but really we want to make sure that these are all learnable from data